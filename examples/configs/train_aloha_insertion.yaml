defaults:
  - env_vars: env_vars # Loads env_vars.yaml for paths and entity
  - _self_

seed: 42
window_size: 8         # T_obs (Observation history length)
action_window_size: 1  # T_act (Action chunk length)

# Training Hyperparameters
goal_window_size: 8
eval_window_size: 8
batch_size: 2048
epochs: 1000
eval_freq: 10
eval_on_env_freq: 200
num_env_evals: 100
num_final_evals: 100
num_final_eval_per_goal: 1
sequentially_select: false
goal_dim: 0
visual_input: false

vqvae_load_dir: "vq_bet_runs/checkpoints/vqbet_aloha_insertion/2025-11-25/0" # Ensure this path is correct
wandb:
  project: "vq-bet-policy"
  entity: "dhruv-101-iit-delhi"

device: cuda
optim:
  lr: 5.5e-5
  weight_decay: 2e-4
  betas: [0.9, 0.999]

env:
  obs_dim: 28           # D_obs (State-only dimension)
  act_dim: 14           # D_act (Raw action dimension)
  goal_dim: 0
  gym:
    _target_: aloha_insertion_env.ALOHAWRAPPER
    id: AlohaInsertion-eval-v0
    goal_dim: ${env.goal_dim}
data:
  _target_: dataset.get_lerobot_train_val
  repo_id: ${env_vars.datasets.aloha_insertion_dataset}
  window_size: ${window_size}
  action_window_size: ${action_window_size}
  visual_input: false
  goal_conditional: future
  future_seq_len: ${goal_window_size}
  min_future_sep: ${action_window_size}
  vqbet_get_future_action_chunk: true

save_every: 10
save_path: "${env_vars.save_path}/checkpoints/vqbet_aloha_insertion/${now:%Y-%m-%d}/${now:%H-%M-%S}"
load_path: null

model:
  _target_: vq_behavior_transformer.BehaviorTransformer
  obs_dim: ${env.obs_dim}
  act_dim: 8                 # ⬅️ FIX: Must match VQ-VAE's n_latent_dims (8)
  goal_dim: ${env.goal_dim}
  obs_window_size: ${window_size}
  act_window_size: ${action_window_size}
  sequentially_select: false
  visual_input: false
  finetune_resnet: false

  gpt_model:
    _target_: vq_behavior_transformer.GPT
    config:
      _target_: vq_behavior_transformer.GPTConfig
      block_size: 9          # ⬅️ FIX: T_obs (8) + T_act (1) = 9
      input_dim: ${env.obs_dim}
      n_layer: 6
      n_head: 6
      n_embd: 128            # ⬅️ FIX: Standardized embedding dimension
      # Added n_codes here, which is often required for VQ-BET/Decision Transformer structures
      n_codes: 16 

  vqvae_model:
    _target_: vqvae.VqVae
    input_dim_h: ${action_window_size} # 1
    input_dim_w: ${env.act_dim}        # 14
    n_latent_dims: 8                   # VQ-VAE latent dimension
    vqvae_n_embed: 16                  # ⬅️ VQ-VAE codebook size (n_codes in GPT config should match this)
    vqvae_groups: 2
    eval: true
    device: ${device}
    load_dir: ${vqvae_load_dir}

  offset_loss_multiplier: 100

goal_fn:
  _target_: aloha_insertion_env.get_goal_fn
  data_directory: ${env_vars.datasets.aloha_insertion_dataset}
  goal_conditional: ${data.goal_conditional}
  seed: ${seed}
  train_fraction: 0.95
  goal_seq_len: ${goal_window_size}
  unconditional: false
  goal_dim: ${goal_dim}
  visual_input: ${visual_input}
